{"componentChunkName":"component---src-pages-examples-sql-json-content-parent-file-id-jsx","path":"/examples/sql/b7a92b3f-acd8-53ec-a778-e84e6a838ca5/","result":{"data":{"jsonContent":{"id":"c0c523b3-d74c-516d-884e-b38010aba48c","content":"CREATE TABLE source\r\n(\r\n    id          bigint,\r\n    user_id     bigint,\r\n    name        STRING\r\n) WITH (\r\n      'connector' = 'mysql-x',\r\n      'url' = 'jdbc:mysql://ip:3308/tudou?useSSL=false',\r\n      'table-name' = 'kudu1',\r\n      'username' = 'username',\r\n      'password' = 'password'\r\n      ,'scan.fetch-size' = '2'\r\n      ,'scan.query-timeout' = '10'\r\n\r\n      ,'scan.polling-interval' = '3000' --间隔轮训时间。非必填(不填为离线任务)，无默认\r\n\r\n      ,'scan.parallelism' = '3' -- 并行度\r\n--       ,'scan.fetch-size' = '2' -- 每次从数据库中fetch大小。默认：1024条\r\n--       ,'scan.query-timeout' = '10' -- 数据库连接超时时间。默认：不超时\r\n\r\n      ,'scan.partition.column' = 'id' -- 多并行度读取的切分字段，多并行度下必需要设置。无默认\r\n      ,'scan.partition.strategy' = 'mod' -- 数据分片策略。默认：range，如果并行度大于1，且是增量任务或者间隔轮询，则会使用mod分片\r\n\r\n      ,'scan.increment.column' = 'id' -- 增量字段名称，必须是表中的字段。非必填，无默认\r\n--       ,'scan.increment.column-type' = 'int'  -- 增量字段类型。非必填，无默认\r\n      ,'scan.start-location' = '100' -- 增量字段开始位置,如果不指定则先同步所有，然后在增量。非必填，无默认，如果没配置scan.increment.column，则不生效\r\n\r\n      ,'scan.restore.columnname' = 'id' -- 开启了cp，任务从sp/cp续跑字段名称。如果续跑，则会覆盖scan.start-location开始位置，从续跑点开始。非必填，无默认\r\n--       ,'scan.restore.columntype' = 'int' -- 开启了cp，任务从sp/cp续跑字段类型。非必填，无默认\r\n      );\r\n\r\nCREATE TABLE sink\r\n(\r\n    id bigint,\r\n    user_id bigint,\r\n    name string\r\n) WITH (\r\n      'connector' = 'hdfs-x'\r\n      ,'path' = 'hdfs://ns/user/hive/warehouse/tudou.db/kudu_txt'\r\n      ,'fileName' = 'pt=1'\r\n      ,'properties.hadoop.user.name' = 'root'\r\n      ,'properties.dfs.ha.namenodes.ns' = 'nn1,nn2'\r\n      ,'properties.fs.defaultFS' = 'hdfs://ns'\r\n      ,'properties.dfs.namenode.rpc-address.ns.nn2' = 'ip:9000'\r\n      ,'properties.dfs.client.failover.proxy.provider.ns' = 'org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider'\r\n      ,'properties.dfs.namenode.rpc-address.ns.nn1' = 'ip:9000'\r\n      ,'properties.dfs.nameservices' = 'ns'\r\n      ,'properties.fs.hdfs.impl.disable.cache' = 'true'\r\n      ,'properties.fs.hdfs.impl' = 'org.apache.hadoop.hdfs.DistributedFileSystem'\r\n      ,'defaultFS' = 'hdfs://ns'\r\n      ,'fieldDelimiter' = ','\r\n      ,'encoding' = 'utf-8'\r\n      ,'maxFileSize' = '10485760'\r\n      ,'nextCheckRows' = '20000'\r\n      ,'writeMode' = 'overwrite'\r\n      ,'fileType' = 'text'\r\n\r\n      ,'sink.parallelism' = '3'\r\n      );\r\n\r\ninsert into sink\r\nselect *\r\nfrom source u;\r\n"}},"pageContext":{"id":"c0c523b3-d74c-516d-884e-b38010aba48c","parent__id":"b7a92b3f-acd8-53ec-a778-e84e6a838ca5","__params":{"parent__id":"b7a92b3f-acd8-53ec-a778-e84e6a838ca5"}}},"staticQueryHashes":["1197112220","1410458087","527733040","63159454"]}